HEAD
#  Proyecto ETL Comercializadora de EnergÃ­a - AWS Data Lake
Pipeline de procesamiento de datos en AWS para comercializadora de energÃ­a



##  DescripciÃ³n General

Este proyecto implementa un pipeline de datos para una comercializadora de energÃ­a que recopila informaciÃ³n desde archivos CSV generados por un sistema transaccional. La soluciÃ³n automatiza el proceso de ingestiÃ³n, transformaciÃ³n, catalogaciÃ³n, consulta y carga hacia un Data Warehouse en AWS Redshift utilizando una arquitectura moderna basada en servicios administrados y buenas prÃ¡cticas de gobierno de datos.

##  Objetivo del Proyecto

Automatizar el flujo de datos provenientes de archivos CSV generados por el sistema de la compaÃ±Ã­a, para:

- Centralizar los datos en un **Data Lake** en Amazon S3.
- Transformarlos y almacenarlos en **formato Parquet**.
- Catalogarlos mediante **AWS Glue Crawlers**.
- Ejecutar consultas con **Amazon Athena**.
- Cargarlos en un **Data Warehouse (Amazon Redshift)**.
- Aplicar control de acceso y permisos mediante **AWS Lake Formation**.
- Orquestar todo el flujo con **AWS Step Functions**.
- Desplegar toda la infraestructura automÃ¡ticamente con **AWS CloudFormation**.


##  Servicios AWS Utilizados

| Servicio              | PropÃ³sito                                                                 |
|-----------------------|--------------------------------------------------------------------------|
| Amazon S3             | Almacenamiento raw y processed (particionado por fecha)                 |
| AWS Glue              | Crawlers para catÃ¡logo, Jobs para transformaciÃ³n y carga a Redshift     |
| Amazon Athena         | Consulta de datos transformados mediante SQL desde Python               |
| Amazon Redshift       | Almacenamiento final en Data Warehouse                                  |
| AWS Lake Formation    | Gobierno de datos, control de acceso y permisos                         |
| AWS Step Functions    | OrquestaciÃ³n del pipeline ETL                                           |
| AWS IAM               | GestiÃ³n de roles y permisos                                             |
| AWS CloudFormation    | Infraestructura como cÃ³digo (IaC) para desplegar toda la arquitectura   |

##  Estructura del Repositorio

```
energia-etl-aws/
â”œâ”€â”€ cloudformation/
â”‚   â””â”€â”€ datalake.yml             # Plantilla IaC completa del proyecto
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ transformar_clientes_parquet.py
â”‚   â”œâ”€â”€ transformar_proveedores_parquet.py
â”‚   â”œâ”€â”€ transformar_transacciones_parquet.py
â”‚   â”œâ”€â”€ load_clientes_s3_to_redshift.py
â”‚   â”œâ”€â”€ load_proveedores_s3_to_redshift.py
â”‚   â””â”€â”€ load_transacciones_s3_to_redshift.py
â”œâ”€â”€ athena/
â”‚   â””â”€â”€ queries.py               # Consultas SQL bÃ¡sicas desde Python con boto3
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/clientes/...
â”‚   â”œâ”€â”€ raw/proveedores/...
â”‚   â””â”€â”€ raw/transacciones/...
â”œâ”€â”€ README.md                    # DocumentaciÃ³n completa
â””â”€â”€ requirements.txt             # Dependencias de Python
```

##  Infraestructura como CÃ³digo (IaC)

Todo el pipeline es desplegable mediante el archivo [`cloudformation/datalake.yml`](cloudformation/datalake.yml). Esto incluye:

- Bucket S3 para las zonas `raw/`, `processed/`, `scripts/`, `temp/`, `athena-query-results/`
- Bases de datos Glue `energia_raw_cf`, `energia_etl_db`
- Crawlers para raw y processed
- Jobs de Glue (transformaciÃ³n y carga)
- MÃ¡quina de estado Step Functions
- Roles IAM (Glue, Redshift, Lambda, Step Functions)
- Lambda para ejecutar DDL en Redshift
- Permisos y registro en Lake Formation

Para desplegar:

```
aws cloudformation deploy   --template-file cloudformation/datalake.yml   --stack-name energia-etl-completo   --capabilities CAPABILITY_NAMED_IAM   --parameter-overrides Environment=dev BucketName=energia-etl-datalake-andrestwd ...
```

## ðŸ”„ OrquestaciÃ³n del Pipeline

- El pipeline estÃ¡ controlado por una **state machine** en AWS Step Functions.
- Define la ejecuciÃ³n secuencial de los Glue Jobs:
  1. TransformaciÃ³n de datos de proveedores, clientes y transacciones.
  2. Carga hacia Redshift.
- Se detiene ante errores y permite reinicios fÃ¡ciles.

##  Consultas SQL en Athena

Consulta de los datos procesados con Athena vÃ­a Python (ejecutadas desde `boto3`):

```sql
-- Total de ventas por tipo de energÃ­a
SELECT tipo_energia, SUM(cantidad_comprada * precio) AS total_ventas
FROM transacciones
WHERE tipo_transaccion = 'venta'
GROUP BY tipo_energia;

-- Conteo de clientes por ciudad
SELECT ciudad, COUNT(*) AS total_clientes
FROM clientes
GROUP BY ciudad;
```

Los resultados se almacenan en:
`s3://energia-etl-datalake-andrestwd/athena-query-results/`

##  Permisos, PolÃ­ticas y Gobierno (Lake Formation)

1. **Registro del recurso**:
   - Bucket registrado como recurso en Lake Formation.
2. **Permisos sobre base de datos y tablas**:
   - `LakeFormationAdmin`: permisos `ALL` sobre S3 y Glue DB.
   - `ReportViewer`: permisos `SELECT` y `DESCRIBE` sobre:
     - `energia_etl_db`: clientes, proveedores.
     - `energia_raw_cf`: transacciones.
3. **IAM Roles utilizados**:
   - `AWSGlueServiceRole-ETL`
   - `LakeFormationAdmin`
   - `ReportViewer`
   - `AthenaQueryExecutionRole`
   - `glue-job-role-dev`
   - `glue-crawler-role-dev`

##  Flujo PeriÃ³dico / AutomatizaciÃ³n

Aunque actualmente la ejecuciÃ³n es **manual (on demand)**, este pipeline puede automatizarse con:

- **Eventos de carga en S3**
- **CloudWatch Events o EventBridge**
- **Triggers en Glue**
- **EjecuciÃ³n programada de Step Functions**

##  Puntos Plus (Bonus)

| Requisito                                 | Implementado |
|------------------------------------------|--------------|
| Infraestructura como cÃ³digo (IaC)        | âœ…           |
| Gobierno con Lake Formation              | âœ…           |
| Carga final en Amazon Redshift           | âœ…           |

##  Requisitos

- AWS CLI configurado
- Python 3.9+
- Boto3
- Zip y herramientas bÃ¡sicas de empaquetado

##  Autores

- AndrÃ©s agudelo
- Prueba tÃ©cnica - EvaluaciÃ³n para ingenierÃ­a de datos